###https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html
### model
model_name_or_path: model/DeepSeek-R1-Distill-Qwen-14B
trust_remote_code: true

### method
stage: pt
do_train: true
finetuning_type: lora
template: qwen
#LoRA 微调中的 dropout 率。
# 防止小数据集过拟合
lora_dropout: 0.05

#rank推荐16,24,32，从小到大常识
#通过将原始权重矩阵分解为两个低秩矩阵，显著减少可训练参数量和计算资源消耗‌
#当降低lora_rank（减少可训练参数量）时，需适当提高学习率以保持训练效果‌
#学习率决定了每次参数更新的幅度，较高的值可能导致训练不稳定或错过最优解，较低的值虽稳定但收敛速度较慢‌
lora_rank: 12

#应用 LoRA 方法的模块名称。使用逗号分隔多个模块，使用 all 指定所有模块。
lora_target: q_proj,k_proj,v_proj

#LoRA 缩放系数。一般情况下为 lora_rank * 2
lora_alpha: 24

#LoRA+ 学习率比例(λ = ηB/ηA)。 ηA, ηB 分别是 adapter matrices A 与 B 的学习率。
#loraplus_lr_ratio

#LoRA+ 嵌入层的学习率。
#loraplus_lr_embedding

#是否使用秩稳定 LoRA (Rank-Stabilized LoRA)。
use_rslora: True

#是否使用权重分解 LoRA（Weight-Decomposed LoRA）。
#DoRA （Weight-Decomposed Low-Rank Adaptation）提出尽管 LoRA 大幅降低了推理成本，但这种方式取得的性能与全量微调之间仍有差距。
#DoRA 将权重矩阵分解为大小与单位方向矩阵的乘积，并进一步微调二者（对方向矩阵则进一步使用 LoRA 分解），从而实现 LoRA 与 Full Fine-tuning 之间的平衡。
use_dora: False

#是否初始化 PiSSA 适配器。
#在 LoRA 中，适配器矩阵 A 由 kaiming_uniform 初始化，而适配器矩阵 B 则全初始化为0。这导致一开始的输入并不会改变模型输出并且使得梯度较小，收敛较慢。 PiSSA 通过奇异值分解直接分解原权重矩阵进行初始化，其优势在于它可以更快更好地收敛。
pissa_init: True

#PiSSA 中 FSVD 执行的迭代步数。使用 -1 将其禁用。
#pissa_iter: 16

#是否将 PiSSA 适配器转换为正常的 LoRA 适配器。
pissa_convert: False

#是否创建一个具有随机初始化权重的新适配器。
create_new_adapter: True

### dataset
dataset: c4_demo
dataset_dir: data/aics/pretrain
cutoff_len: 2048
#max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: save/DeepSeek-R1-Distill-Qwen-14B/lora/pretrain
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 3.0e-5
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
#bf16: true
# 启用显存优化技术
fp16: true
# 8位优化器节省显存
#optim: adamw_8bit
ddp_timeout: 180000000
resume_from_checkpoint: null

### eval
# eval_dataset: c4_demo
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500


###加速训练
#FlashAttention 能够加快注意力机制的运算速度，同时减少对内存的使用。
flash_attn: fa2

#Unsloth 框架支持 Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen等大语言模型并且支持 4-bit 和 16-bit 的 QLoRA/LoRA 微调，该框架在提高运算速度的同时还减少了显存占用。
#use_unsloth: True

#Liger Kernel 是一个大语言模型训练的性能优化框架, 可有效地提高吞吐量并减少内存占用。
#enable_liger_kernel: True